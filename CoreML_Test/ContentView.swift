//
//  ContentView.swift â€” iOS 17 deprecations fixed
//  CoreML_Test
//
//  Updated: 2025/10/29
//

import SwiftUI
import UIKit
import AVFoundation
import Vision
import CoreML
import Combine

// MARK: - View

struct ContentView: View {
    @StateObject private var vm = CameraVM()
    
    var body: some View {
        ZStack {
            CameraPreview(session: vm.session) { layer in
                vm.previewLayer = layer
                // èµ·å‹•ã¯ modelReady ã®ã¿ï¼ˆã“ã“ã§ã¯ start ã—ãªã„ï¼‰
            }
            .ignoresSafeArea()

            PillOverlay(boxes: vm.boxes)

            VStack {
                HStack { Spacer(); CountBadge(count: vm.boxes.count).padding() }
                Spacer()
            }

            if vm.cameraDenied {
                VStack(spacing: 12) {
                    Text("è¨­å®š > ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ > ã‚«ãƒ¡ãƒ© ã§æœ¬ã‚¢ãƒ—ãƒªã‚’è¨±å¯ã—ã¦ãã ã•ã„")
                        .multilineTextAlignment(.center)
                    Button("è¨­å®šã‚’é–‹ã") {
                        if let url = URL(string: UIApplication.openSettingsURLString) {
                            UIApplication.shared.open(url)
                        }
                    }
                    .buttonStyle(.borderedProminent)
                }
                .padding()
                .background(.ultraThinMaterial)
                .clipShape(RoundedRectangle(cornerRadius: 12))
                .padding()
            }
        }
        .onAppear {
            // åˆå›ä½“é¨“å‘ä¸Šï¼šèµ·å‹•ç›´å¾Œã«æ¨©é™ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’å‡ºã—ã¦ãŠãï¼ˆæœªæ±ºå®šã®ã¿ï¼‰
            if AVCaptureDevice.authorizationStatus(for: .video) == .notDetermined {
                AVCaptureDevice.requestAccess(for: .video) { _ in }
            }
        }
        .onReceive(NotificationCenter.default.publisher(for: UIApplication.willResignActiveNotification)) { _ in
            vm.stop()
        }
        .onReceive(NotificationCenter.default.publisher(for: UIApplication.didBecomeActiveNotification)) { _ in
            if vm.modelReady, vm.previewLayer != nil { vm.start() }
        }
        // iOS17ã® onChange æ–°ã‚·ã‚°ãƒãƒãƒ£ï¼ˆ2å¼•æ•°ï¼‰
        .onChange(of: vm.modelReady) { _, ready in
            if ready, vm.previewLayer != nil {
                // ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆå®‰å®šã®ãŸã‚å°‘ã—é…ã‚‰ã›ã‚‹ï¼ˆãƒãƒ³ã‚°æŠ‘åˆ¶ï¼‰
                DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { vm.start() }
            }
        }
        .alert("ã‚¨ãƒ©ãƒ¼", isPresented: Binding(get: { vm.errorMessage != nil },
                                           set: { if !$0 { vm.errorMessage = nil } })) {
            Button("OK", role: .cancel) { vm.errorMessage = nil }
        } message: {
            Text(vm.errorMessage ?? "")
        }
    }
}

// MARK: - ViewModel

final class CameraVM: NSObject, ObservableObject, AVCaptureVideoDataOutputSampleBufferDelegate {

    // å‡ºåŠ›
    @Published var boxes: [DrawBox] = []
    @Published var cameraDenied = false
    @Published var modelReady = false
    @Published var errorMessage: String?

    // ã‚«ãƒ¡ãƒ©
    let session = AVCaptureSession()
    fileprivate var previewLayer: AVCaptureVideoPreviewLayer?
    private let sessionQueue = DispatchQueue(label: "pill.session.queue")

    // Capture / Infer ã‚’åˆ†é›¢
    private let captureQueue = DispatchQueue(label: "pill.capture.queue", qos: .userInitiated)
    private let inferQueue   = DispatchQueue(label: "pill.infer.queue", qos: .userInitiated)

    // Vision
    private var request: VNCoreMLRequest?
    private var didSetupRequest = false  // äºŒé‡æ§‹ç¯‰ã‚¬ãƒ¼ãƒ‰
    private let confThresh: VNConfidence = 0.10

    // å®Ÿè¡Œåˆ¶å¾¡
    private let inferSemaphore = DispatchSemaphore(value: 1)
    private var lastInferTime = CFAbsoluteTimeGetCurrent()
    private var targetFPS: Double = 3.0 { didSet { inferInterval = 1.0 / max(1.0, targetFPS) } }
    private var inferInterval: CFTimeInterval = 1.0 / 3.0 // â‰ˆ3fps
    private var lastUIUpdate = CFAbsoluteTimeGetCurrent()
    private let uiInterval: CFTimeInterval = 0.10    // UI ã¯æœ€å¤§ 10fps

    // ãƒ”ã‚¯ã‚»ãƒ«ãƒãƒƒãƒ•ã‚¡å¯¸æ³•ï¼ˆå›è»¢å¾Œï¼‰
    private var lastPixelBufferSize: CGSize?

    // ä½¿ç”¨ã‚«ãƒ¡ãƒ©ã®å‘ãï¼ˆã‚ªãƒªã‚¨ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¤‰æ›ã§ä½¿ç”¨ï¼‰
    private var devicePosition: AVCaptureDevice.Position = .back

    override init() {
        super.init()
        // ã‚«ãƒ¡ãƒ©æ§‹æˆã¯ãƒ¡ã‚¤ãƒ³ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„
        sessionQueue.async { [weak self] in
            self?.setupCamera()
        }
        // ãƒ¢ãƒ‡ãƒ«éåŒæœŸãƒ­ãƒ¼ãƒ‰ï¼ˆç«¯æœ«çŠ¶æ…‹ã«åˆã‚ã›ã¦è¨ˆç®—è³‡æºã‚’é¸æŠï¼‰
        setupVision()
    }

    // MARK: Camera

    func start() {
        switch AVCaptureDevice.authorizationStatus(for: .video) {
        case .authorized:
            sessionQueue.async {
                if !self.session.isRunning {
                    self.session.startRunning()
                    print("[INFO] session.startRunning()")
                }
            }
        case .notDetermined:
            AVCaptureDevice.requestAccess(for: .video) { [weak self] ok in
                guard let self else { return }
                if ok {
                    self.sessionQueue.async {
                        if !self.session.isRunning { self.session.startRunning() }
                        print("[INFO] session.startRunning() after permission")
                    }
                } else {
                    DispatchQueue.main.async { self.cameraDenied = true }
                }
            }
        default:
            DispatchQueue.main.async { self.cameraDenied = true }
        }
    }

    func stop() {
        sessionQueue.async { [weak self] in
            guard let self, self.session.isRunning else { return }
            self.session.stopRunning()
            print("[INFO] session.stopRunning()")
        }
    }

    private func setupCamera() {
        session.beginConfiguration()
        session.sessionPreset = .vga640x480

        // èƒŒé¢åºƒè§’
        guard let cam = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back),
              let input = try? AVCaptureDeviceInput(device: cam) else {
            session.commitConfiguration()
            DispatchQueue.main.async { self.errorMessage = "ã‚«ãƒ¡ãƒ©å…¥åŠ›ã‚’åˆæœŸåŒ–ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚" }
            return
        }
        devicePosition = cam.position
        if session.canAddInput(input) { session.addInput(input) }

        do {
            try cam.lockForConfiguration()
            // 12fps ãƒ­ãƒƒã‚¯ï¼ˆé…å»¶ã¨ç™ºç†±ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
            if cam.activeFormat.videoSupportedFrameRateRanges.contains(where: { $0.maxFrameRate >= 12 }) {
                cam.activeVideoMinFrameDuration = CMTime(value: 1, timescale: 12)
                cam.activeVideoMaxFrameDuration = CMTime(value: 1, timescale: 12)
            }
            if cam.isSmoothAutoFocusSupported { cam.isSmoothAutoFocusEnabled = true }
            if cam.isLowLightBoostSupported { cam.automaticallyEnablesLowLightBoostWhenAvailable = true }
            if cam.isFocusModeSupported(.continuousAutoFocus) { cam.focusMode = .continuousAutoFocus }
            if cam.isExposureModeSupported(.continuousAutoExposure) { cam.exposureMode = .continuousAutoExposure }
            if cam.isWhiteBalanceModeSupported(.continuousAutoWhiteBalance) { cam.whiteBalanceMode = .continuousAutoWhiteBalance }
            cam.unlockForConfiguration()
        } catch {
            print("[WARN] Camera lockForConfiguration failed: \(error.localizedDescription)")
        }

        let output = AVCaptureVideoDataOutput()
        output.alwaysDiscardsLateVideoFrames = true
        output.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA]
        output.setSampleBufferDelegate(self, queue: captureQueue)
        if session.canAddOutput(output) { session.addOutput(output) }
        if let conn = output.connection(with: .video) {
            if #available(iOS 17.0, *) {
                // iOS17+: å›è»¢è§’ãƒ™ãƒ¼ã‚¹
                if conn.isVideoRotationAngleSupported(90) {
                    conn.videoRotationAngle = 90
                }
            } else if conn.isVideoOrientationSupported {
                // iOS16-: æ—§API
                conn.videoOrientation = .portrait
            }
        }
        session.commitConfiguration()
    }

    // MARK: Vision/CoreML

    // ãƒãƒ³ãƒ‰ãƒ«å†…ã® .mlmodelc ã‚’å„ªå…ˆã€‚ãªã‘ã‚Œã° .mlpackage ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    private func urlForCompiledModel() -> URL? {
        let bundle = Bundle.main
        if let urls = bundle.urls(forResourcesWithExtension: "mlmodelc", subdirectory: nil),
           let u = urls.first {
            print("[INFO] Found .mlmodelc:", u.lastPathComponent)
            return u
        }
        print("[WARN] No .mlmodelc. Try .mlpackage â†’ compile.")
        if let pkgs = bundle.urls(forResourcesWithExtension: "mlpackage", subdirectory: nil),
           let p = pkgs.first {
            do {
                let compiled = try MLModel.compileModel(at: p)
                print("[INFO] Compiled .mlpackage â†’ .mlmodelc:", compiled.lastPathComponent)
                return compiled
            } catch {
                print("[ERROR] Compile .mlpackage failed:", error.localizedDescription)
                DispatchQueue.main.async { self.errorMessage = "CoreMLãƒ¢ãƒ‡ãƒ«(.mlpackage)ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸã€‚" }
                return nil
            }
        }
        print("[ERROR] No .mlmodelc/.mlpackage in bundle.")
        DispatchQueue.main.async { self.errorMessage = "CoreMLãƒ¢ãƒ‡ãƒ«ãŒãƒãƒ³ãƒ‰ãƒ«å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚" }
        return nil
    }

    private func setupVision() {
        print("[DEBUG] setupVision(): staged MLModel.load start (no blocking)")
        guard let url = urlForCompiledModel() else { return }

        // ğŸ” ä½é›»åŠ›ãƒ¢ãƒ¼ãƒ‰ã§ tier ã®å„ªå…ˆã‚’åˆ‡æ›¿
        let lowPower = ProcessInfo.processInfo.isLowPowerModeEnabled
        let tiers: [MLComputeUnits] = lowPower
        ? [.cpuOnly, .cpuAndGPU, .all]   // çœé›»åŠ›å„ªå…ˆ
        : [.all, .cpuAndGPU, .cpuOnly]   // æ€§èƒ½å„ªå…ˆ

        let timeoutSec: Double = 6
        var completed = false

        func tryLoad(at index: Int) {
            guard index < tiers.count, !completed else { return }
            let cfg = MLModelConfiguration()
            cfg.computeUnits = tiers[index]
            MLModel.load(contentsOf: url, configuration: cfg) { [weak self] result in
                guard let self, !completed else { return }
                switch result {
                case .success(let model):
                    completed = true
                    print("[DEBUG] MLModel.load(\(tiers[index])) âœ… (lowPower:\(lowPower))")
                    self.buildVNModelAndRequest(model)
                case .failure(let err):
                    print("[WARN] load(\(tiers[index])) failed:", err.localizedDescription)
                    tryLoad(at: index + 1)
                }
            }
            // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§æ¬¡ tier ã‚’è©¦ã™ï¼ˆæˆåŠŸ/å¤±æ•—ã§ completed ãŒç«‹ã¤ï¼‰
            DispatchQueue.global(qos: .userInitiated).asyncAfter(deadline: .now() + timeoutSec) {
                if !completed { tryLoad(at: index + 1) }
            }
        }
        tryLoad(at: 0)
    }

    // VNCoreMLModel ã‚’ä½œã£ã¦ Request æ§‹ç¯‰
    private func buildVNModelAndRequest(_ model: MLModel) {
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            guard let self else { return }
            do {
                let vn = try VNCoreMLModel(for: model)
                print("[DEBUG] VNCoreMLModel built âœ…. Building requestâ€¦")
                DispatchQueue.main.async {
                    self.buildVisionRequest(vn)
                }
            } catch {
                print("[ERROR] VNCoreMLModel build failed:", error.localizedDescription)
                DispatchQueue.main.async { self.errorMessage = "VNCoreMLModel ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚" }
            }
        }
    }

    private func buildVisionRequest(_ vnModel: VNCoreMLModel) {
        // äºŒé‡å®Ÿè¡Œã‚¬ãƒ¼ãƒ‰
        if didSetupRequest {
            print("[DEBUG] buildVisionRequest skipped (already set)")
            return
        }
        didSetupRequest = true

        let req = VNCoreMLRequest(model: vnModel) { [weak self] request, _ in
            guard let self else { return }
            let obs = (request.results as? [VNRecognizedObjectObservation]) ?? []

            let now = CFAbsoluteTimeGetCurrent()
            guard now - self.lastUIUpdate >= self.uiInterval else { return }
            self.lastUIUpdate = now

            // ãƒ¬ã‚¤ãƒ¤ãƒ¼å¤‰æ›ï¼‹UI æ›´æ–°ã¯ãƒ¡ã‚¤ãƒ³ã¸
            DispatchQueue.main.async {
                guard let pl = self.previewLayer else { return }
                self.boxes = self.convertToLayerRects(obs, previewLayer: pl)
            }
        }

        // âš ï¸ usesCPUOnly ã¯ iOS17 ã§éæ¨å¥¨ â†’ ä½¿ã‚ãªã„
        // çœé›»åŠ›å¯¾å¿œã¯ MLModelConfiguration.computeUnits å´ã§è§£æ±ºæ¸ˆã¿

        // YOLO ã®å‰å‡¦ç†ã«åˆã‚ã›ã¦èª¿æ•´ï¼ˆå­¦ç¿’ãŒ letterbox ãªã‚‰ .scaleFit ãŒè¿‘ã„ï¼‰
        req.imageCropAndScaleOption = .scaleFill

        self.request = req
        print("[DEBUG] request built âœ…")

        // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆBGï¼‰
        inferQueue.async { [weak self] in
            guard let self else { return }
            self.warmUp()
            print("[DEBUG] warmUp done (background)")
            DispatchQueue.main.async {
                self.modelReady = true
                print("[DEBUG] modelReady = true (after warmUp) âœ…")
            }
        }
    }

    // åˆå›ã ã‘ãƒ€ãƒŸãƒ¼å…¥åŠ›ã§ performï¼ˆMetal/BNNS åˆæœŸåŒ–ï¼‰
    private func warmUp() {
        guard let req = self.request else { return }
        if let pb = Self.makePixelBuffer(width: 320, height: 320) {
            let h = VNImageRequestHandler(cvPixelBuffer: pb, orientation: .up, options: [:])
            _ = try? h.perform([req]) // çµæœã¯æ¨ã¦ã‚‹
        }
    }

    // MARK: æ¨è«–

    func captureOutput(_ output: AVCaptureOutput, didOutput sb: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard modelReady, let request = self.request else { return }

        // å®Ÿãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ”ã‚¯ã‚»ãƒ«å¯¸æ³•ã‚’æ›´æ–°ï¼ˆå›è»¢ã«å¿œã˜ã¦å…¥æ›¿ï¼‰
        if let pixel = CMSampleBufferGetImageBuffer(sb) {
            let w = CVPixelBufferGetWidth(pixel)
            let h = CVPixelBufferGetHeight(pixel)
            #if compiler(>=5.9)
            if #available(iOS 17.0, *) {
                // videoRotationAngle ã§ 90/270 ã¯ç¸¦æŒã¡ï¼ˆå¹…é«˜å…¥æ›¿ï¼‰
                let rot = Int(connection.videoRotationAngle) % 360
                switch rot {
                case 90, 270:
                    lastPixelBufferSize = CGSize(width: h, height: w)
                default:
                    lastPixelBufferSize = CGSize(width: w, height: h)
                }
            } else {
                switch connection.videoOrientation {
                case .portrait, .portraitUpsideDown:
                    lastPixelBufferSize = CGSize(width: h, height: w)
                case .landscapeLeft, .landscapeRight:
                    lastPixelBufferSize = CGSize(width: w, height: h)
                @unknown default:
                    lastPixelBufferSize = CGSize(width: w, height: h)
                }
            }
            #else
            switch connection.videoOrientation {
            case .portrait, .portraitUpsideDown:
                lastPixelBufferSize = CGSize(width: h, height: w)
            case .landscapeLeft, .landscapeRight:
                lastPixelBufferSize = CGSize(width: w, height: h)
            @unknown default:
                lastPixelBufferSize = CGSize(width: w, height: h)
            }
            #endif
        }

        let now = CFAbsoluteTimeGetCurrent()
        guard now - lastInferTime >= inferInterval else { return }
        lastInferTime = now

        guard inferSemaphore.wait(timeout: .now()) == .success else { return }

        inferQueue.async { [weak self] in
            defer { self?.inferSemaphore.signal() }
            guard let self = self else { return }

            // ã‚ªãƒªã‚¨ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ "å›è»¢è§’Ã—ã‚«ãƒ¡ãƒ©ä½ç½®" ã‹ã‚‰æ±ºå®š
            let orient = self.cgImageOrientation(for: connection, devicePosition: self.devicePosition)

            // sampleBuffer ã‹ã‚‰ç›´æ¥ handler ã‚’ç”Ÿæˆï¼ˆä½™è¨ˆãªã‚³ãƒ”ãƒ¼ã‚’é¿ã‘ã‚‹ï¼‰
            let handler = VNImageRequestHandler(cmSampleBuffer: sb, orientation: orient, options: [:])
            do {
                try handler.perform([request])
            } catch {
                print("[ERROR] VN perform:", error.localizedDescription)
            }
        }
    }

    // MARK: Orientation (rotation-angle based for iOS17)

    private func cgImageOrientation(for connection: AVCaptureConnection, devicePosition: AVCaptureDevice.Position) -> CGImagePropertyOrientation {
        let isFront = (devicePosition == .front)

        var rotationAngle: CGFloat = 0
        if #available(iOS 17.0, *) {
            rotationAngle = CGFloat(connection.videoRotationAngle) // 0 / 90 / 180 / 270
        } else {
            switch connection.videoOrientation {
            case .portrait:            rotationAngle = 90
            case .portraitUpsideDown:  rotationAngle = 270
            case .landscapeRight:      rotationAngle = 0
            case .landscapeLeft:       rotationAngle = 180
            @unknown default:          rotationAngle = 90
            }
        }

        switch Int(rotationAngle) % 360 {
        case 0:   return isFront ? .upMirrored    : .up
        case 90:  return isFront ? .leftMirrored  : .right
        case 180: return isFront ? .downMirrored  : .down
        case 270: return isFront ? .rightMirrored : .left
        default:  return isFront ? .leftMirrored  : .right
        }
    }

    // MARK: åº§æ¨™å¤‰æ›ï¼ˆPreviewLayer API ã‚’å„ªå…ˆï¼‰

    private func convertToLayerRects(
        _ obs: [VNRecognizedObjectObservation],
        previewLayer pl: AVCaptureVideoPreviewLayer
    ) -> [DrawBox] {
        guard lastPixelBufferSize != nil else { return [] }

        var rects: [DrawBox] = []
        for o in obs.sorted(by: { $0.confidence > $1.confidence }).prefix(40) {
            guard o.confidence >= self.confThresh else { continue }

            // Vision æ­£è¦åŒ–ï¼ˆå·¦ä¸‹åŸç‚¹ï¼‰â†’ å·¦ä¸ŠåŸç‚¹ã¸åè»¢
            let v = o.boundingBox
            let normTop = CGRect(x: v.minX,
                                 y: 1 - v.minY - v.height,
                                 width: v.width,
                                 height: v.height)

            // å…¬å¼APIã§ 0..1 â†’ ãƒ¬ã‚¤ãƒ¤åº§æ¨™ã¸
            let layerRect = pl.layerRectConverted(fromMetadataOutputRect: normTop)

            let area = layerRect.width * layerRect.height
            guard area >= 16 * 16 else { continue }

            rects.append(DrawBox(rect: layerRect,
                                 score: o.confidence,
                                 label: o.labels.first?.identifier ?? "pill"))
        }
        return rects
    }

    // MARK: Utils

    private static func makePixelBuffer(width: Int, height: Int) -> CVPixelBuffer? {
        var pb: CVPixelBuffer?
        let attrs = [
            kCVPixelBufferCGImageCompatibilityKey: kCFBooleanTrue as Any,
            kCVPixelBufferCGBitmapContextCompatibilityKey: kCFBooleanTrue as Any
        ] as CFDictionary
        CVPixelBufferCreate(kCFAllocatorDefault, width, height, kCVPixelFormatType_32BGRA, attrs, &pb)
        return pb
    }
}

// MARK: - Preview Layer

final class PreviewView: UIView {
    override class var layerClass: AnyClass { AVCaptureVideoPreviewLayer.self }
    var videoPreviewLayer: AVCaptureVideoPreviewLayer { layer as! AVCaptureVideoPreviewLayer }
}

struct CameraPreview: UIViewRepresentable {
    let session: AVCaptureSession
    let onReady: (AVCaptureVideoPreviewLayer) -> Void

    func makeUIView(context: Context) -> PreviewView {
        let v = PreviewView()
        let l = v.videoPreviewLayer
        l.videoGravity = .resizeAspectFill
        l.session = session
        onReady(l)
        return v
    }

    func updateUIView(_ uiView: PreviewView, context: Context) {
        uiView.videoPreviewLayer.frame = uiView.bounds
    }
}

// MARK: - Overlayï¼ˆè»½é‡ Canvas + ãƒ©ãƒ™ãƒ«ï¼‰

struct PillOverlay: View {
    let boxes: [DrawBox]
    var body: some View {
        Canvas { ctx, _ in
            let sorted = boxes.sorted { $0.score > $1.score }
            let maxRects = sorted.prefix(40)

            for b in maxRects where (b.rect.width * b.rect.height) >= 16 * 16 {
                // æ 
                let p = Path(CGRect(x: b.rect.minX, y: b.rect.minY,
                                    width: b.rect.width, height: b.rect.height))
                ctx.stroke(p, with: .color(.green), lineWidth: 2)

                // ãƒ©ãƒ™ãƒ«
                let text = "\(b.label) \(Int(b.score * 100))%"
                let layout = Text(AttributedString(text))
                    .font(.caption)
                    .foregroundStyle(.green)
                ctx.draw(layout, at: CGPoint(x: b.rect.minX + 4, y: b.rect.minY + 12), anchor: .topLeading)
            }
        }
        .ignoresSafeArea()
        .allowsHitTesting(false)
    }
}

// MARK: - ã‚«ã‚¦ãƒ³ãƒˆè¡¨ç¤º

struct CountBadge: View {
    let count: Int
    var body: some View {
        Text("Pills: \(count)")
            .font(.headline)
            .padding(10)
            .background(.ultraThinMaterial)
            .clipShape(RoundedRectangle(cornerRadius: 10))
    }
}

// MARK: - æç”»ç”¨ãƒ¢ãƒ‡ãƒ«

struct DrawBox: Hashable {
    var rect: CGRect
    var score: VNConfidence
    var label: String
}

// MARK: - Preview

#Preview {
    ContentView()
}
